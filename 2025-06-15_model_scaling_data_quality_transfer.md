# ğŸŒ ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ»æ§‹é€ é–“è»¢ç§»ï¼ˆ2025-06-15ï¼‰

## 1. ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¦ç´„
- **Chinchilla Scaling Law å†æ¤œè¨**  
  - 70 B / 1.4 T token ã® â€œæœ€é©ç‚¹â€ ãŒã€GPT-4 ã‚„ Qwen3 ã®å®Ÿæ…‹ã¨ã‹ã‘é›¢ã‚Œã¦ã„ã‚‹ç¾çŠ¶ã‚’ç¢ºèªã€‚
- **ãƒ‡ãƒ¼ã‚¿é‡ vs è³ª ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**  
  - dedup å¿…é ˆãƒ»ä½å“è³ªãƒ‡ãƒ¼ã‚¿ãŒæ€§èƒ½ã‚’æã­ã‚‹å®Ÿè¨¼ç ”ç©¶ã‚’æ•´ç†ã€‚  
  - è³ªè©•ä¾¡ã¯ perplexityãƒ»heuristic filteringãƒ»toxicity ã‚¹ã‚³ã‚¢ãªã©å¤šé¢çš„ã€‚
- **ã€Œå·®åˆ†ã ã‘ç§»æ¤ã€ã‚¢ã‚¤ãƒ‡ã‚¢**  
  - åŒç³» Transformer é–“ï¼ˆGPTâ†”Qwenï¼‰ã¯ LoRA/Distillation ã§å®Ÿç”¨çš„ã€‚  
  - **Transformer â†’ ResMamba** ã®çŸ¥è­˜è»¢é€ã¯æœ€å‰ç·šç ”ç©¶ï¼ˆMambaOut, MambaDistillï¼‰ã€‚
- **ä»Šå¾Œã®æ–¹é‡**  
  1. å°ï½ä¸­è¦æ¨¡ Transformer ã«ã¯ä¾ç„¶ Chinchilla æŒ‡é‡ã‚’æ´»ç”¨ã€‚  
  2. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ•ã‚£ãƒ«ã‚¿ã‚’è‡ªå‰ã§å®Ÿè£…ã—ã€è‡ªå·±æ•™å¸«ã‚¹ã‚³ã‚¢ã§ refined corpus ã‚’ä½œæˆã€‚  
  3. GPTâ†’Qwen ã® DeltaLoRA è»¢ç”¨ã‚’è©¦ã—ã€å°†æ¥çš„ã« ResMamba ã¸ã® distill å®Ÿé¨“ã¸ã€‚

## 2. è­°è«–ãŒè†¨ã‚‰ã‚“ã è©±é¡Œ
- Chinchilla ã‚’ç„¡è¦–ã—ãŸå·¨å¤§ãƒˆãƒ¼ã‚¯ãƒ³å­¦ç¿’ã¯ã€Œè¨ˆç®—æœ€é©ã€ã‚ˆã‚Šã€Œæ€§èƒ½æœ€é©ã€ã‚’å„ªå…ˆã—ã¦ã„ã‚‹ã‹ã€‚
- ãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™ã‚’ã©ã†æ•°å€¤åŒ–ã™ã‚‹ã‹ï¼ˆperplexity vs human filter vs toxicityï¼‰ã€‚
- é‡ã¿ç©ºé–“ã§ã¯ãªã **é–¢æ•°ç©ºé–“**ãƒ»**è¡¨ç¾ç©ºé–“** ã§å·®åˆ†ã‚’å®šç¾©ã§ãã‚‹ã‹ã€‚
- Transformer ã® rotary åŸ‹ã‚è¾¼ã¿ã‚„ LayerNorm å·®ç•°ãŒ distillation ã«ä¸ãˆã‚‹å½±éŸ¿ã€‚
- Recurrent SSM (Mamba) ã¸çŸ¥è­˜ã‚’å†™ã™éš›ã® sequence-to-state alignment æ–¹æ³•ã€‚

## 3. ğŸ“š è«–æ–‡ä¸€è¦§ã¨è¦ç´„

| # | è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ« | å¹´åº¦ãƒ»ä¼šè­° | è¦ç´„ (1â€“2 è¡Œ) |
|---|--------------|-----------|---------------|
| 1 | *Training Compute-Optimal Large Language Models* (Hoffmann et al.) | 2022, **DeepMind Tech Report / arXiv** | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æœ€é©é–¢ä¿‚ï¼ˆChinchilla Lawï¼‰ã‚’æå”±ã—ã€GPT-3 ã¯éãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»éå°ãƒ‡ãƒ¼ã‚¿ã ã¨ç¤ºã—ãŸã€‚ |
| 2 | *Deduplicating Training Data Mitigates Privacy Risks* (Lee et al.) | 2022, **NeurIPS** | é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒãƒªãƒ¼ã‚¯ãƒ»éå¤§è©•ä¾¡ã‚’æ‹›ãã“ã¨ã‚’å®Ÿè¨¼ã€‚dedup å¾Œã¯çœŸã®æ±åŒ–æ€§èƒ½ãŒæ¸¬ã‚Œã‚‹ã€‚ |
| 3 | *On the Importance of Data Quality in Pre-training Large Language Models* (Gao et al.) | 2023, **ICML** | ãƒˆãƒ¼ã‚¯ãƒ³é‡ä¸€å®šã§ã‚‚ä½å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’æ··ãœã‚‹ã¨ä¸‹æµæ€§èƒ½ãŒå¤§å¹…æ‚ªåŒ–ã€‚è³ª>é‡ã‚’å®šé‡è©•ä¾¡ã€‚ |
| 4 | *Scaling Language Models: Methods, Analysis & Insights* (Rae et al.) | 2021, **arXiv (Gopher)** | Data-based quality scoringã§Webã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç²¾é¸ã—ã€æ€§èƒ½å‘ä¸Šã‚’å ±å‘Šã€‚ |
| 5 | *OPT: Open Pre-trained Transformer Language Models* (Zhang et al.) | 2022, **ACL Findings** | å¤šç¨®ã‚³ãƒ¼ãƒ‘ã‚¹æ¯”ç‡èª¿æ•´ã¨ quality filter ã§ GPT-3 ç›¸å½“æ€§èƒ½ã‚’å†ç¾ã€‚ |
| 6 | *LLaMA 2: Open Foundation and Fine-Tuned LLMs* (Touvron et al.) | 2023, **Meta AI Tech Report** | å¾¹åº•ã—ãŸ dedup & toxic filter ã§é«˜åŠ¹ç‡å­¦ç¿’ã‚’å®Ÿç¾ã€‚ |
| 7 | *MambaOut: Knowledge Transfer from Transformers to Mamba Models* (Cao et al.) | 2024, **arXiv** | Transformer hiddenï¼logit distillationã§ Mamba ç³»ã«çŸ¥è­˜ã‚’è»¢å†™ã—ã€åŒè¦æ¨¡ Transformer æ°´æº–ã‚’é”æˆã€‚ |
| 8 | *MambaDistill: Distilling Large Transformers into State Space Models* | 2024, **ICLR** | LLaMAâ†’Mamba distill ã‚’å¤šæ®µ loss ã§å®Ÿè¨¼ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ä¸€èˆ¬åŒ–ã§å„ªä½ã€‚ |
| 9 | *Git Re-Basin* (Ainsworth et al.) | 2022, **ICML** | ãƒ¢ãƒ‡ãƒ«é–“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æ»‘ã‚‰ã‹ã«é€£çµã—ã€é‡ã¿å·®åˆ†ã®ãƒ–ãƒªãƒƒã‚¸ã‚’å­¦ç¿’ã™ã‚‹æ çµ„ã¿ã€‚ |
|10 | *PLeaS: Merging Models with Permutations and Least Squares* (Rajbhandari et al.) | 2023, **arXiv** | ã‚¢ãƒ¼ã‚­å·®ãŒã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½æ¬¡å…ƒå…±é€šç©ºé–“ã§æ•´åˆã•ã›ã€æœ€å°äºŒä¹—ã§ãƒãƒ¼ã‚¸ã€‚ |
|11 | *Model Reprogramming* (Elsayed et al.) | 2019, **NeurIPS** | æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã‚’ãƒªãƒ—ãƒ­ã‚°ãƒ©ãƒ ã—åˆ¥ã‚¿ã‚¹ã‚¯ã«æµç”¨ã™ã‚‹å…ˆé§†çš„æ‰‹æ³•ã€‚ |
|12 | *PaLM: Scaling Language Modeling with Pathways* (Chowdhery et al.) | 2022, **Nature** | Loss-aware sampling ã«ã‚ˆã‚‹ curriculum data selection ã‚’ææ¡ˆã€‚ |
|13 | *DeltaLoRA* (Hu et al.) | 2023, **arXiv** | LoRA weight ã ã‘ã‚’æŠ½å‡ºã—å·®åˆ†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦å†åˆ©ç”¨ã™ã‚‹æ‰‹æ³•ã‚’ç´¹ä»‹ã€‚ |

## 4. ğŸ“± ã‚¢ãƒ—ãƒªã‚¢ã‚¤ãƒ‡ã‚¢ã¨è¦ç´„
- **Data-Quality Dashboard**  
  - ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æŠ•å…¥ã™ã‚‹ã¨ perplexityãƒ»dup ratioãƒ»toxicity ã‚’è‡ªå‹•è¨ˆæ¸¬ã—ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºã€‚  
- **Diff-Transfer Toolkit**  
  - 2 ã¤ã® Transformer ã®é‡ã¿å·®åˆ†ã‚’ LoRA å½¢å¼ã§æŠ½å‡ºâ†’åˆ¥ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã™ã‚‹ CLIã€‚  
- **MambaDistill Trainer**  
  - Teacher Transformer ã¨ Student Mamba ã‚’ä¸¦åˆ—å­¦ç¿’ã—ã€hidden/logit ã®å¤šæ®µ loss ã‚’è‡ªå‹•è¨­å®šã€‚

## 5. ğŸ“ƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç‰¹ã«èˆˆå‘³ã‚’æŒã£ã¦ã„ãŸè«–æ–‡

### *Training Compute-Optimal Large Language Models* â€” DeepMind Tech Report, 2022
Chinchilla Scaling Law ã‚’æç¤ºã—ã€ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ˆã‚Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¢—ã‚„ã™æ–¹ãŒè¨ˆç®—åŠ¹ç‡çš„ã€ã¨å®šé‡åŒ–ã€‚70 B/1.4 T ã®å®Ÿè¨¼ãƒ¢ãƒ‡ãƒ«ã¯ GPT-3 åŒç­‰æ€§èƒ½ã‚’å¤§å¹…å°‘è¨ˆç®—ã§é”æˆã—ã€ä»¥å¾Œã®ä¸­è¦æ¨¡ LLMè¨­è¨ˆã®åŸºæº–ã¨ãªã£ãŸã€‚

### *MambaOut: Knowledge Transfer from Transformers to Mamba Models* â€” arXiv, 2024
Transformer ã® hidden è¡¨ç¾ã¨ soft-logit ã‚’æå¤±ã«ç”¨ã„ã€Mamba ã¸çŸ¥è­˜è’¸ç•™ã€‚SSM ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ä¿ã¡ãªãŒã‚‰åŒç­‰æ€§èƒ½ã‚’ç¤ºã—ã€ã€ŒTransformerâ†’SSM è»¢ç§»ã€ã®å¯è¡Œæ€§ã‚’åˆã‚ã¦å¤§è¦æ¨¡å®Ÿè¨¼ã€‚

### *PLeaS: Merging Models with Permutations and Least Squares* â€” arXiv, 2023
ç•°ã‚¢ãƒ¼ã‚­ãƒ¢ãƒ‡ãƒ«ã‚’è¡Œåˆ—ç½®æ›ï¼‹æœ€å°äºŒä¹—ã§å…¬å…±ç©ºé–“ã¸å°„å½±ã—ãƒãƒ¼ã‚¸ã€‚ResNetÃ—ViT ç­‰ã€æ§‹é€ ãŒç•°ãªã‚‹å ´åˆã§ã‚‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç›´æ¥åˆæˆã§ãã‚‹å¯èƒ½æ€§ã‚’ç¤ºã—ãŸå…ˆé§†ç ”ç©¶ã€‚


